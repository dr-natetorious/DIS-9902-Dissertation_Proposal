# Hyperscale ML Training

## Strong Scaling in Deep Learning (2021)

Oyama, Y., Maruyama, N., Dryden, N., McCarthy, E., Harrington, P., Balewski, J., Matsuoka, S., Nugent, P., & Van Essen, B. (2021). The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs With Hybrid Parallelism. IEEE Transactions on Parallel and Distributed Systems, Parallel and Distributed Systems, IEEE Transactions on, IEEE Trans. Parallel Distrib. Syst, 32(7), 1641–1652. https://doi.org/10.1109/TPDS.2020.3047974. [StrongScaling](StrongScaling.pdf).

## Towards Generic and Efficient Elastic Training (2020)

Xie, L., Zhai, J., Wu, B., Wang, Y., Zhang, X., Sun, P., & Yan, S. (2020). Elan: Towards Generic and Efficient Elastic Training for Deep Learning. 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS), Distributed Computing Systems (ICDCS), 2020 IEEE 40th International Conference on, ICDCS, 78–88. https://doi.org/10.1109/ICDCS47774.2020.00018. [Elan.pdf](Elan.pdf).

## A Secure Algorithm for Deep Learning Training (2020)

Prashar, A., & Salinas Monroy, S. A. (2020). A Secure Algorithm for Deep Learning Training under GAN Attacks. 2020 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI), Communications, Computing, Cybersecurity, and Informatics (CCCI), 2020 International Conference On, 1–6. https://doi.org/10.1109/CCCI49893.2020.9256566. [SecureAlgorithm.pdf](SecureAlgorithm.pdf).

## HITL simulation in ROS-based UAV (2020)

Moreac, E., Abdali, E. M., Berry, F., Heller, D., & Diguet, J.-P. (2020). Hardware-in-the-loop simulation with dynamic partial FPGA reconfiguration applied to computer vision in ROS-based UAV. 2020 International Workshop on Rapid System Prototyping (RSP), Rapid System Prototyping (RSP), 2020 IEEE International Workshop On, 1–7. https://doi.org/10.1109/RSP51120.2020.9244863. [HILT_ROS-based_UAV](HILT_ROS-based_UAV.pdf).

## Distributed Training of Deep Learning Models (2020)

Langer, M., He, Z., Rahayu, W., & Xue, Y. (2020). Distributed Training of Deep Learning Models: A Taxonomic Perspective. IEEE Transactions on Parallel and Distributed Systems, Parallel and Distributed Systems, IEEE Transactions on, IEEE Trans. Parallel Distrib. Syst, 31(12), 2802–2818. https://doi.org/10.1109/TPDS.2020.3003307. [CommunicationModels](CommunicationModels.pdf).

## Language Models are Few-Shot Learners (2020)

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. [arvix](https://arxiv.org/abs/2005.14165). [LanguageModels.pdf](LanguageModels.pdf)

## Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity (2022)

William Fedus, Barret Zoph, Noam Shazeer; 23(120):1−39, 2022. [arxiv](https://arxiv.org/pdf/2101.03961.pdf). [SwitchTransformers.pdf](SwitchTransformers.pdf).
