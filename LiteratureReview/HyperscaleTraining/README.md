# Hyperscale ML Training

## Market Trajectory (2021)

Global Data Center Accelerators Market Trajectory & Analytics 2020-2027: GPU, CPU, FPGA, ASIC, Deep Learning Training, Public Cloud Interface, Enterprise Interface. Plus Company Updates. March 5, 2021. Accessed April 15, 2022. [EBSCO](https://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=edsinc&AN=edsinc.A653973707&site=eds-live&scope=site). [Global_Data_Center_Accelerator](Global_Data_Center_Accelerator.pdf)

## Strong Scaling in Deep Learning (2021)

Oyama, Y., Maruyama, N., Dryden, N., McCarthy, E., Harrington, P., Balewski, J., Matsuoka, S., Nugent, P., & Van Essen, B. (2021). The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs With Hybrid Parallelism. IEEE Transactions on Parallel and Distributed Systems, Parallel and Distributed Systems, IEEE Transactions on, IEEE Trans. Parallel Distrib. Syst, 32(7), 1641–1652. https://doi.org/10.1109/TPDS.2020.3047974. [StrongScaling](StrongScaling.pdf).

## Towards Generic and Efficient Elastic Training (2020)

Xie, L., Zhai, J., Wu, B., Wang, Y., Zhang, X., Sun, P., & Yan, S. (2020). Elan: Towards Generic and Efficient Elastic Training for Deep Learning. 2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS), Distributed Computing Systems (ICDCS), 2020 IEEE 40th International Conference on, ICDCS, 78–88. https://doi.org/10.1109/ICDCS47774.2020.00018. [Elan.pdf](Elan.pdf).

## A Secure Algorithm for Deep Learning Training (2020)

Prashar, A., & Salinas Monroy, S. A. (2020). A Secure Algorithm for Deep Learning Training under GAN Attacks. 2020 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI), Communications, Computing, Cybersecurity, and Informatics (CCCI), 2020 International Conference On, 1–6. https://doi.org/10.1109/CCCI49893.2020.9256566. [SecureAlgorithm.pdf](SecureAlgorithm.pdf).

## An Efficient and Non-Intrusive GPU Scheduling Framework (2020)

Wang, S., Gonzalez, O. J., Zhou, X., Williams, T., Friedman, B. D., Havemann, M., & Woo, T. (2020). An Efficient and Non-Intrusive GPU Scheduling Framework for Deep Learning Training Systems. SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, High Performance Computing, Networking, Storage and Analysis, SC20: International Conference for, SC, 1–13. https://doi.org/10.1109/SC41405.2020.00094. [NonIntrusiveGPUSchedule](NonIntrusiveGPUSchedule.pdf).

## Energy-Efficient Hardware Accelerators (2021)

Lee, J., & Yoo, H. (2021). An Overview of Energy-Efficient Hardware Accelerators for On-Device Deep-Neural-Network Training. IEEE Open Journal of the Solid-State Circuits Society, Solid-State Circuits Society, IEEE Open Journal of the, IEEE Open J. Solid-State Circuits Soc, 1, 115–128. https://doi.org/10.1109/OJSSCS.2021.3119554. [OnDeviceHardwareAccelerator](OnDeviceHardwareAccelerator.pdf).

## Continuous Training and Deployment (2021)

Prapas, I., Derakhshan, B., Mahdiraji, A. R., & Markl, V. (2021). Continuous Training and Deployment of Deep Learning Models. Datenbank-Spektrum, 21(3), 203. https://doi.org/10.1007/s13222-021-00386-8. [ContinousTrainingDeployment](ContinousTrainingDeployment.pdf).

## Accelerating Continual Learning on Edge FPGA (2021)

Piyasena, D., Lam, S.-K., & Wu, M. (2021). Accelerating Continual Learning on Edge FPGA. 2021 31st International Conference on Field-Programmable Logic and Applications (FPL), Field-Programmable Logic and Applications (FPL),2021 31st International Conference on, FPL, 294–300. https://doi.org/10.1109/FPL53798.2021.00059. [ContinualLearningAtEdge](ContinualLearningAtEdge.pdf).

