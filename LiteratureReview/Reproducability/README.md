# Reproducability challenges in ML

## The Challenge of Reproducible ML (2021)

E. Rivera-Landos, F. Khomh and A. Nikanjam, "The Challenge of Reproducible ML: An Empirical Study on The Impact of Bugs," 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), 2021, pp. 1079-1088, doi: 10.1109/QRS54544.2021.00116. [EmpiricalStudy.pdf](EmpiricalStudy.pdf).

The authors state that only 25% of deep learning papers contain code, and most repositories are unusable. One study found that 9% of surveyed papers **could** be reproduced, not wheather the results match.

They talk about the need for seeding data, which is good suggestion.

## Analyzing the analyzers (2018)

Qiu, L., Wang, Y., & Rubin, J. (2018). Analyzing the analyzers: FlowDroid/IccTA, AmanDroid, and DroidSafe. Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis, 176–186. https://doi.org/10.1145/3213846.3213873

This paper also appeared in [TIM-8101 Principals of CS Week 5](https://github.com/dr-natetorious/TIM-8101-Principals_of_Computer_Science/tree/master/Week5_Conferences).

## Should We Strive to Make Science Bias-Free (2021)

Hudson, R. (2021). Should We Strive to Make Science Bias-Free? A Philosophical Assessment of the Reproducibility Crisis. Journal for General Philosophy of Science, 52(3), 389–405. https://doi.org/10.1007/s10838-020-09548-w. [ScienceBiasFree.pdf](ScienceBiasFree.pdf).

## Reproducibility in deep learning algorithms (2020)

Tomaszewski, J. E., Ward, A. D., Li, W., & Chen, W. (2020). Reproducibility in deep learning algorithms for digital pathology applications: a case study using the CAMELYON16 datasets. Proceedings of SPIE, 11603, 1160318. https://doi.org/10.1117/12.2581996. 